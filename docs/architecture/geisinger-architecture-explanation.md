# Geisinger Enterprise Agentic Architecture
## High-Level Architecture Guide
**Version 1.0 | January 2025**

---

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Component Breakdown](#component-breakdown)
3. [Information Flow](#information-flow)
4. [Key Design Patterns](#key-design-patterns)
5. [Implementation Modules](#implementation-modules)
6. [Deployment Considerations](#deployment-considerations)

---

## Architecture Overview

### Philosophy
The Geisinger Agentic Architecture implements **true agency** (not workflows) through:
- **Dynamic Agent Loop**: Autonomous plan â†’ act â†’ verify â†’ iterate cycle
- **MCP-First Integration**: All enterprise connectors as MCP servers
- **Self-Verification First**: Agents validate before human review (60-80% burden reduction)
- **Domain Agnostic Core**: Same foundation across clinical, legal, product, and all domains

### Architectural Layers (11 Total + Meta)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 8: META-BLUEPRINT (Universal Constitution)       â”‚ â† ALWAYS ACTIVE
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Core Agent Orchestration (The Brain)          â”‚
â”‚  Layer 2: Memory & Context Management                   â”‚
â”‚  Layer 3: Tool Use Framework (Primary Building Blocks)  â”‚
â”‚  Layer 4: Agent Perceptions (Push/Pull Awareness)       â”‚
â”‚  Layer 5: Agent Interactions (Communication)            â”‚
â”‚  Layer 6: Domain Knowledge Base                         â”‚
â”‚  Layer 7: Strategic Knowledge (BluePrints)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 9: Human-in-the-Loop (Tiered Oversight)         â”‚
â”‚  Layer 10: Guardrails & Safety (Multi-Layer Defense)   â”‚
â”‚  Layer 11: Evaluation & Observability (Quality Gates)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Breakdown

### ðŸ¤– LAYER 1: Core Agent Orchestration Engine
**Purpose**: The autonomous reasoning system - the "brain" of the agent

#### The Agent Loop (Heart of Agency)
```
1. GATHER Context
   â””â”€ From: Memory, Perceptions, BluePrints
   
2. PLAN Action
   â””â”€ Consult: Domain & Strategic Knowledge
   â””â”€ Select: Tools and approach
   
3. ACT (Execute Tools)
   â””â”€ Via: MCP clients â†’ MCP servers
   
4. SELF-VERIFY
   â””â”€ Against: Meta-Blueprint, Domain policies
   â””â”€ Check: Completeness, consistency, safety
   
5. ITERATE/ADAPT
   â””â”€ If not complete â†’ Loop back to GATHER
   â””â”€ If complete â†’ Proceed to HITL
```

#### Key Capabilities
- **Dynamic Planning**: No hardcoded workflows; adapts based on context
- **Tool Discovery**: Learns which tools work best for tasks
- **Failure Recovery**: Tries alternatives when primary approach fails
- **Subagent Spawning**: Creates specialized workers for complex tasks
- **Self-Correction**: Adjusts plans based on verification results

---

### ðŸ’¾ LAYER 2: Memory & Context Management
**Purpose**: Maintains state and manages the 200K token context budget

#### Components

**Working Memory (50K token allocation)**
- Active conversation context (last 3 turns always loaded)
- Current task state and progress
- Recent tool results (5-minute TTL)
- Temporary reasoning conclusions
- Error history and lessons learned within session

**Long-Term Persistent Memory**
- Session summaries for continuity
- Learned patterns and successful strategies
- Entity relationships (patient, project, case)
- Performance metrics and outcomes

**Context Budget Manager**
- Total: 200K tokens (Claude Opus 4.1)
- Always-loaded: ~12K tokens (6% - Meta, task, safety)
- Dynamic pool: ~188K tokens (94% - flexible allocation)
- Emergency reserve: 8K tokens (4% - overflow protection)

**Auto-Compaction Service**
- Triggers at 80% capacity (160K tokens)
- Compression ratio target: 10:1
- Preserves: Goals, decisions, action items, safety-critical data
- Discards: Debug info, duplicate data, intermediate calculations

#### Context Loading Strategy
```
ALWAYS LOADED (~12K):
â”œâ”€ Meta-Blueprint core principles (2K)
â”œâ”€ Current task definition (1K)
â”œâ”€ User role & permissions (1K)
â”œâ”€ Active conversation (last 3 turns, ~3K)
â”œâ”€ Working memory summary (5K)

ON-DEMAND RETRIEVAL:
â”œâ”€ Domain BluePrints (loaded when classified)
â”œâ”€ Historical context (retrieved via tools)
â”œâ”€ Specific policy documents (as needed)
â”œâ”€ Long documentation (fetched, not kept)
```

---

### ðŸ”§ LAYER 3: Tool Use Framework
**Purpose**: Tools are the agent's "hands" - primary means of perception and action

#### Architecture

**Tool Registry (MCP-Based)**
```yaml
tool_entry:
  tool_id: UUID
  mcp_endpoint: "mcp://server.geisinger.org/tool-name"
  owner: "team@geisinger.org"
  version: "2.1.0"
  risk_tier: "MEDIUM"
  requires_consent: true
  rate_limit: "1000 req/min"
  sla:
    availability: "99.9%"
    response_p99: "1000ms"
```

**Tool Discovery Engine**
- Agents browse tool catalog at startup
- Track success rates per tool
- Learn optimal tool combinations
- Identify alternatives for failures

**Tool Execution Controller**
- Dynamic tool selection (not predefined sequences)
- Parallel execution when beneficial
- Automatic retry with backoff
- Result integration and caching

**Tool Verifier**
- Post-execution validation
- Output schema checking
- Result completeness verification
- Safety constraint enforcement

#### Tool Categories
1. **Perception Tools** (Pull data)
   - Database queries
   - API reads
   - Document retrieval
   - Search operations

2. **Action Tools** (Change state)
   - Database writes
   - Email sending
   - Calendar operations
   - Document generation

3. **Hybrid Tools** (Both)
   - Workflow systems
   - Communication platforms
   - Collaborative tools

---

### ðŸ‘ï¸ LAYER 4: Agent Perceptions
**Purpose**: Environmental awareness through push and pull mechanisms

#### Push-Based Perceptions (Event-Driven)
```
Event Stream â†’ Filter Rules â†’ Agent Notification
                                  â†“
                    Automatic Context Loading
                                  â†“
                    Trigger Planning Module
```

**Sources**:
- Real-time system events
- Threshold-based alerts (e.g., "Lab result critical")
- State changes (e.g., "Patient admitted")
- Scheduled monitoring checks
- Subscription feeds

#### Pull-Based Perceptions (Query-Driven)
```
Agent Needs Info â†’ Formulate Query
                       â†“
            Select Appropriate Tool
                       â†“
            Execute and Cache Result
                       â†“
            Integrate into Working Memory
```

**Mechanisms**:
- Direct queries (specific data requests)
- Search operations (fuzzy finding)
- Aggregations (summary statistics)
- Verification checks (fact validation)
- Context expansion (related data discovery)

---

### ðŸ’¬ LAYER 5: Agent Interactions
**Purpose**: Multi-modal communication and collaboration with humans

#### Components

**Conversational Interface**
- Natural language dialogue
- Optional chain-of-thought visibility
- Source attribution with citations
- Confidence indicators
- Clarification loops for ambiguity

**Proactive Recommendations**
```json
{
  "recommendation": "Schedule code review for PR #1234",
  "justification": {
    "data_sources": ["GitHub API", "Team Calendar"],
    "blueprint_alignment": "Code Review Policy v2.1",
    "expected_outcome": "Faster merge, fewer bugs",
    "confidence": 0.87,
    "alternatives": ["Defer to next sprint", "Auto-merge with tests"]
  },
  "user_actions": ["approve", "modify", "reject", "discuss"]
}
```

**Explainability Engine**
- Decision reasoning chains
- Data source transparency
- Policy references
- Alternative options considered
- Confidence factors (supporting & limiting)

---

### ðŸ“– LAYER 6: Domain Knowledge Base
**Purpose**: Authoritative, versioned domain expertise

#### Knowledge Categories
- **Policies**: Hard rules (MUST follow)
- **Guidelines**: Best practices (SHOULD follow)
- **Procedures**: Step-by-step processes (HOW to)
- **References**: Factual information
- **Examples**: Canonical cases
- **Exceptions**: Edge case handling

#### Version Control
```
Knowledge Update â†’ Version Increment
                       â†“
            Change Documentation
                       â†“
            Agent Notification
                       â†“
            Gradual Migration
```

---

### ðŸ§  LAYER 7: Strategic Knowledge (BluePrints)
**Purpose**: Institutional wisdom - the agent's "extended mind" and constitution

#### BluePrint Structure
```yaml
blueprint:
  name: "Clinical Decision Support"
  type: "domain_specific"
  priority: "high"
  
  policies:
    - "Always verify patient identity"
    - "Check allergies before prescribing"
    - "Document all clinical reasoning"
  
  guidelines:
    - "Prefer evidence-based treatments"
    - "Consider patient preferences"
    - "Escalate uncertainty to physicians"
  
  best_practices:
    - approach: "Gather full medical history"
    - approach: "Consider differential diagnoses"
    - approach: "Validate with clinical guidelines"
  
  knowledge:
    key_facts: ["Clinical decision trees", "Drug interactions"]
    references: ["UpToDate", "Clinical pathways"]
    examples: ["Case studies", "Successful treatments"]
```

#### Multi-BluePrint Composition
When agents need multiple expertise areas:
```
Task: "Review contract for clinical trial"
  â†“
Load BluePrints:
â”œâ”€ Legal Compliance
â”œâ”€ Clinical Research
â””â”€ HIPAA Privacy

Resolve Conflicts:
â”œâ”€ Higher priority overrides lower
â”œâ”€ Domain-specific beats general
â”œâ”€ Safety rules ALWAYS win
â””â”€ Log conflicts for review
```

---

### ðŸ“‹ LAYER 8: Meta-Blueprint (Universal Constitution)
**Purpose**: Immutable principles governing ALL agent behavior

#### Core Principles (Never Override)
1. **Human Safety**: Wellbeing above all else
2. **Transparency**: All reasoning must be explainable
3. **Privacy**: Respect confidentiality always
4. **Compliance**: Follow all laws and regulations
5. **Learning**: Continuous improvement from experience

#### Problem-Solving Heuristics
- Break complex tasks into steps
- When uncertain, gather more information
- Verify assumptions before proceeding
- Document reasoning for auditability
- Consider multiple alternatives
- Learn from failures within session

#### Escalation Triggers (Auto-Stop)
- Confidence below 70%
- Potential policy violation detected
- Safety or risk concerns identified
- Novel situation without precedent
- Explicit user request for human

#### Operational Boundaries
- âŒ Never make irreversible changes without approval
- âŒ Never access data without authorization
- âŒ Never hide or obscure failures
- âŒ Never proceed when safety uncertain
- âŒ Never violate user trust

---

### ðŸ‘¥ LAYER 9: Human-in-the-Loop Integration
**Purpose**: Efficient oversight with minimal burden through self-verification

#### Self-Verification Pipeline (BEFORE Human Review)
```
Agent Output
    â†“
Internal Validation Suite:
â”œâ”€ Policy Compliance (Blueprint alignment)
â”œâ”€ Completeness Check (all requirements met)
â”œâ”€ Consistency Check (no contradictions)
â”œâ”€ Quality Assessment (meets standards)
â””â”€ Confidence Evaluation (certainty level)
    â†“
All Checks Pass?
â”œâ”€ NO â†’ Return to Agent Loop (with specific feedback)
â””â”€ YES â†’ Determine Review Tier
    â†“
Review Tier Assessment
```

#### HITL Tiers (Risk-Based)

**Tier 1: No Review Needed**
- Low-impact reads (e.g., "Show calendar")
- Status checks (e.g., "Check ticket status")
- Summaries of public data
- **Action**: Auto-proceed

**Tier 2: Passive Review**
- Medium-impact operations
- Reversible writes (e.g., "Draft email")
- Internal data queries
- **Action**: Proceed, audit later (10% sampling)

**Tier 3: Active Approval**
- High-impact operations
- Legal drafts
- Scheduling commitments
- Resource allocations
- **Action**: Queue for human approval, wait

**Tier 4: Immediate Escalation**
- Clinical writebacks (e.g., "Submit order")
- Irreversible changes (e.g., "Deploy to production")
- Financial transactions
- Legal filings
- **Action**: Alert human immediately, block execution

#### Efficiency Gains
- **70-80%** reduction in human review needs
- Higher quality when review IS needed
- Clear justification for all review requests
- Batch approval capabilities
- Learning from approval patterns

---

### ðŸ›¡ï¸ LAYER 10: Guardrails & Safety Framework
**Purpose**: Multi-layer defense with agent self-governance

#### Self-Governance Layer (NEW)
Agent monitors its own behavior:
- Deviation from normal patterns
- Unusual tool usage sequences
- Repeated failures on similar tasks
- Confidence degradation over time
- Policy near-misses

**Auto-Corrections**:
- Reset context if confused
- Reduce autonomy if failing
- Request help proactively
- Document issues for review

#### Traditional Safety Layers
```
Layer 1: Input Validation
    â””â”€ Sanitize, validate types, check patterns
    
Layer 2: Planning Validation
    â””â”€ Check proposed plan against policies
    
Layer 3: Execution Monitoring
    â””â”€ Watch tool usage in real-time
    
Layer 4: Output Validation
    â””â”€ Verify final results before delivery
    
Layer 5: Audit Trail
    â””â”€ Complete action history (immutable)
```

#### Prompt Injection Defense
```python
# Structural Separation
[SYSTEM_INSTRUCTION]
Tool: fetch_patient_data
Parameters: {"patient_id": "validated_input"}
[END_SYSTEM_INSTRUCTION]

[USER_CONTENT_QUARANTINE]
{sanitized_user_input}
[END_USER_CONTENT_QUARANTINE]
```

**Defenses**:
- Remove control characters
- Escape special syntax
- Validate parameter types
- Length limits enforced
- Pattern matching for suspicious content
- Tool boundary protection
- Output validation for instruction leakage

---

### ðŸ“Š LAYER 11: Evaluation & Observability
**Purpose**: Continuous validation and transparency

#### Evaluation Components

**Golden Test Suite (Per Domain)**
- Canonical tasks that define success
- Example: "CLIN-001: Patient triage assessment"
- Pass criteria: Correct actions + correct outcomes
- Run frequency: Every commit, nightly, weekly

**Safety Test Suite**
- Prompt injection resistance
- Data leakage prevention
- Authorization compliance
- PHI/PII protection

**Regression Test Suite**
- Every fixed bug â†’ permanent test
- Prevents backsliding
- Runs on every release candidate

**Performance Benchmarks**
- Latency targets (p50, p99)
- Throughput (QPS)
- Token efficiency
- Resource usage

**Red Team Testing**
- Adversarial prompting
- Multi-turn attacks
- Tool abuse attempts
- Authorization bypass attempts

**Shadow Testing**
- Candidate agent mirrors production traffic
- Results compared but not served
- Low-risk validation of changes

#### Metrics Dashboard
```
Real-Time:
â”œâ”€ Current safety score
â”œâ”€ Golden test pass rate
â”œâ”€ Active regressions
â””â”€ Shadow test variance

Trends:
â”œâ”€ Score over time
â”œâ”€ Regression frequency
â”œâ”€ Performance degradation
â””â”€ Safety incident rate

Alerts:
â”œâ”€ Safety score drop >5%
â”œâ”€ New critical regression
â”œâ”€ Red team success
â””â”€ Performance SLA breach
```

---

## Information Flow

### Primary Request Flow
```
1. User Request
    â†“
2. Conversational Interface (Layer 5)
    â†“
3. Input Validation (Layer 10)
    â†“
4. AGENT LOOP BEGINS (Layer 1)
    â”œâ”€ Load Meta-Blueprint (Layer 8) - ALWAYS
    â”œâ”€ Classify domain â†’ Load Domain Blueprints (Layer 7)
    â”œâ”€ Gather Context (from Layer 2, 4, 6)
    â”œâ”€ Plan Action (consult Layer 7, 6)
    â”œâ”€ Execute Tools (via Layer 3 â†’ MCP â†’ External Systems)
    â”œâ”€ Self-Verify (against Layer 8, 10)
    â””â”€ Iterate until complete
    â†“
5. Self-Verification Pipeline (Layer 9)
    â”œâ”€ Policy compliance check
    â”œâ”€ Completeness check
    â”œâ”€ Consistency check
    â””â”€ Confidence assessment
    â†“
6. HITL Tier Assessment (Layer 9)
    â”œâ”€ Tier 1 â†’ Auto-proceed
    â”œâ”€ Tier 2 â†’ Proceed + audit
    â”œâ”€ Tier 3 â†’ Wait for approval
    â””â”€ Tier 4 â†’ Immediate escalation
    â†“
7. Response Generation
    â†“
8. Output Validation (Layer 10)
    â†“
9. User Response (via Layer 5)
    â†“
10. Logging & Metrics (Layer 11)
```

### Tool Execution Flow (MCP-Based)
```
Agent (Layer 1)
    â†“ "I need patient data"
Tool Discovery (Layer 3)
    â†“ Check Tool Registry
Select Tool: "fetch_patient_data"
    â†“
RBAC Check (Cross-Cutting)
    â†“ Verify permissions
Consent Check (if required)
    â†“ Get user consent
MCP Client (Layer 3)
    â†“ Prepare MCP request
MCP Protocol
    â†“ "mcp://ehr.geisinger.org/patient_data"
MCP Server (External)
    â†“ Execute query
External System (EHR)
    â†“ Return data
MCP Server
    â†“ Format response
MCP Client
    â†“ Receive result
Tool Verifier (Layer 3)
    â†“ Validate output
Cache Result (Layer 2)
    â†“ Store with 5-min TTL
Return to Agent (Layer 1)
    â†“ Continue agent loop
```

### Context Management Flow
```
Request Arrives
    â†“
Context Budget Manager checks capacity
    â”œâ”€ < 80% â†’ Normal operation
    â”œâ”€ 80-90% â†’ Warning, start compaction prep
    â”œâ”€ 90-95% â†’ Aggressive compaction
    â””â”€ > 95% â†’ Emergency compaction
    â†“
Load Always-Present Context (~12K)
    â”œâ”€ Meta-Blueprint (2K)
    â”œâ”€ Current task (1K)
    â”œâ”€ Recent conversation (3K)
    â”œâ”€ User context (1K)
    â””â”€ Working memory summary (5K)
    â†“
Retrieve On-Demand Context (as needed)
    â”œâ”€ Domain BluePrints (when classified)
    â”œâ”€ Historical context (via tools)
    â”œâ”€ Specific documents (via tools)
    â””â”€ Additional examples (if uncertain)
    â†“
Execute Agent Loop
    â†“
Update Working Memory
    â†“
Check TTL on cached items
    â””â”€ Expire old data (5-30 min depending on type)
    â†“
If approaching limit (160K):
    â”œâ”€ Trigger auto-compaction
    â”œâ”€ Summarize conversation (10:1 ratio)
    â”œâ”€ Compress tool results
    â””â”€ Move to long-term memory
```

---

## Key Design Patterns

### Pattern 1: MCP-First Integration
**Problem**: Fragmented enterprise integrations, duplicated code
**Solution**: All systems expose MCP servers; all agents use MCP clients

```
Traditional Approach:
Agent â†’ Custom Connector 1 â†’ EHR
     â†’ Custom Connector 2 â†’ JIRA
     â†’ Custom Connector 3 â†’ SharePoint
     (N different integration patterns)

MCP Approach:
Agent â†’ MCP Client â†’ MCP Protocol â†’ MCP Server (EHR)
                                  â†’ MCP Server (JIRA)
                                  â†’ MCP Server (SharePoint)
     (One universal protocol)
```

**Benefits**:
- Single integration pattern
- Reusable across all agents
- Versioned, discoverable
- Security at protocol level
- Easy to add new systems

### Pattern 2: Self-Verification First
**Problem**: Human review bottleneck, low-quality agent outputs
**Solution**: Agents validate their own work before requesting human review

```
Old Pattern:
Agent Output â†’ Human Review â†’ Approve/Reject
(100% human review burden)

New Pattern:
Agent Output â†’ Self-Verification Suite â†’ 70% Auto-Approved
                                      â†’ 30% to Human Review
(70-80% reduction in human burden)
```

**Self-Verification Checks**:
1. Policy compliance (against BluePrints)
2. Completeness (all requirements met)
3. Consistency (no contradictions)
4. Confidence (above threshold)
5. Safety (PHI, authorization)

### Pattern 3: Dynamic Agent Loop (Not Workflows)
**Problem**: Brittle, predefined workflows that can't adapt
**Solution**: Autonomous loop that plans, acts, verifies, iterates

```
Workflow (Brittle):
Step 1 â†’ Step 2 â†’ Step 3 â†’ Done
(Fails if Step 2 doesn't work)

Agent Loop (Adaptive):
Goal â†’ Plan â†’ Act â†’ Verify â†’ Iterate
    â†‘_____________________________|
(Tries alternatives if action fails)
```

**Example**:
```
Goal: "Schedule team meeting"
  â†“
Attempt 1: Check everyone's calendar
  â”œâ”€ Tool fails (calendar API down)
  â””â”€ Iterate: Try alternative approach
  â†“
Attempt 2: Email team for availability
  â”œâ”€ Success: Got responses
  â””â”€ Continue
  â†“
Attempt 3: Book conference room
  â”œâ”€ Room unavailable
  â””â”€ Iterate: Find alternative room
  â†“
Success: Meeting scheduled
```

### Pattern 4: Context as a Budget
**Problem**: Context overflow, important data lost, poor performance
**Solution**: Treat context as managed resource with budgets and compaction

```
Context Management:
â”œâ”€ Total Budget: 200K tokens
â”œâ”€ Always-Loaded: 12K (fixed allocation)
â”œâ”€ Dynamic Pool: 188K (flexible)
â””â”€ Emergency Reserve: 8K

Triggers:
â”œâ”€ 80% â†’ Start compaction prep
â”œâ”€ 90% â†’ Aggressive compaction
â””â”€ 95% â†’ Emergency procedures

Compaction Strategy:
â”œâ”€ Summarize conversations (10:1)
â”œâ”€ Compress tool results
â”œâ”€ Move to long-term memory
â””â”€ Keep safety-critical data
```

### Pattern 5: Multi-Agent Orchestration
**Problem**: Single agent can't handle breadth-heavy or parallel tasks efficiently
**Solution**: Lead agent spawns specialized worker agents

```
Complex Task: "Analyze Q4 performance across all departments"
    â†“
Lead Agent (Orchestrator)
    â”œâ”€ Spawns Worker 1: Finance analysis
    â”œâ”€ Spawns Worker 2: Operations analysis  
    â”œâ”€ Spawns Worker 3: Sales analysis
    â””â”€ Spawns Worker 4: Customer satisfaction analysis
    â†“
Parallel Execution (faster)
    â†“
Lead Agent Integrates Results
    â†“
Final Report
```

**When to Use**:
- Breadth-heavy tasks (multiple domains)
- Time constraints (need speed)
- Cross-domain evidence gathering
- Isolation of concerns for safety

**Limits**:
- Cap fan-out (max 5-10 workers)
- Enforce budgets per worker
- Aggregate confidence scores
- Fail fast on weak signals

---

## Implementation Modules

Based on the architecture, here are the primary implementation modules:

### Module 1: Core Agent Runtime
```
geisinger-agent-core/
â”œâ”€ orchestrator/
â”‚  â”œâ”€ agent_loop.py
â”‚  â”œâ”€ planner.py
â”‚  â”œâ”€ executor.py
â”‚  â””â”€ verifier.py
â”œâ”€ memory/
â”‚  â”œâ”€ working_memory.py
â”‚  â”œâ”€ long_term_memory.py
â”‚  â”œâ”€ context_manager.py
â”‚  â””â”€ compaction_service.py
â””â”€ config/
   â”œâ”€ meta_blueprint.yaml
   â””â”€ agent_config.yaml
```

### Module 2: Tool Framework & MCP Integration
```
geisinger-tools/
â”œâ”€ registry/
â”‚  â”œâ”€ tool_registry.py
â”‚  â”œâ”€ tool_spec.yaml
â”‚  â””â”€ registry_api.py
â”œâ”€ mcp/
â”‚  â”œâ”€ mcp_client.py
â”‚  â”œâ”€ mcp_server_base.py
â”‚  â””â”€ protocol_handlers.py
â”œâ”€ discovery/
â”‚  â”œâ”€ tool_discovery.py
â”‚  â””â”€ capability_matcher.py
â””â”€ execution/
   â”œâ”€ tool_executor.py
   â”œâ”€ parallel_executor.py
   â””â”€ result_cache.py
```

### Module 3: MCP Servers (Enterprise Connectors)
```
geisinger-mcp-servers/
â”œâ”€ ehr_server/
â”‚  â”œâ”€ mcp_ehr_server.py
â”‚  â”œâ”€ ehr_client.py
â”‚  â””â”€ server_config.yaml
â”œâ”€ jira_server/
â”‚  â”œâ”€ mcp_jira_server.py
â”‚  â”œâ”€ jira_client.py
â”‚  â””â”€ server_config.yaml
â”œâ”€ sharepoint_server/
â”‚  â””â”€ ...
â””â”€ shared/
   â”œâ”€ auth_handler.py
   â”œâ”€ rate_limiter.py
   â””â”€ audit_logger.py
```

### Module 4: Perceptions System
```
geisinger-perceptions/
â”œâ”€ push/
â”‚  â”œâ”€ event_listener.py
â”‚  â”œâ”€ alert_processor.py
â”‚  â””â”€ stream_handlers.py
â”œâ”€ pull/
â”‚  â”œâ”€ query_engine.py
â”‚  â”œâ”€ search_service.py
â”‚  â””â”€ aggregation_service.py
â””â”€ integration/
   â””â”€ perception_router.py
```

### Module 5: Interaction Layer
```
geisinger-interactions/
â”œâ”€ conversation/
â”‚  â”œâ”€ chat_interface.py
â”‚  â”œâ”€ dialogue_manager.py
â”‚  â””â”€ clarification_handler.py
â”œâ”€ recommendations/
â”‚  â”œâ”€ proactive_recommender.py
â”‚  â””â”€ justification_builder.py
â””â”€ explainability/
   â”œâ”€ reasoning_tracer.py
   â”œâ”€ citation_manager.py
   â””â”€ confidence_explainer.py
```

### Module 6: Knowledge Management
```
geisinger-knowledge/
â”œâ”€ domain_kb/
â”‚  â”œâ”€ policy_store.py
â”‚  â”œâ”€ guideline_store.py
â”‚  â”œâ”€ procedure_store.py
â”‚  â””â”€ knowledge_api.py
â”œâ”€ blueprints/
â”‚  â”œâ”€ blueprint_loader.py
â”‚  â”œâ”€ blueprint_composer.py
â”‚  â”œâ”€ clinical_bp.yaml
â”‚  â”œâ”€ legal_bp.yaml
â”‚  â””â”€ product_bp.yaml
â””â”€ versioning/
   â””â”€ knowledge_version_control.py
```

### Module 7: HITL System
```
geisinger-hitl/
â”œâ”€ verification/
â”‚  â”œâ”€ self_verification_suite.py
â”‚  â”œâ”€ policy_checker.py
â”‚  â”œâ”€ completeness_checker.py
â”‚  â””â”€ consistency_checker.py
â”œâ”€ approval/
â”‚  â”œâ”€ tier_classifier.py
â”‚  â”œâ”€ approval_queue.py
â”‚  â””â”€ feedback_integrator.py
â””â”€ ui/
   â”œâ”€ approval_dashboard.py
   â””â”€ batch_approval_ui.py
```

### Module 8: Safety & Guardrails
```
geisinger-safety/
â”œâ”€ input_validation/
â”‚  â”œâ”€ sanitizer.py
â”‚  â”œâ”€ prompt_injection_detector.py
â”‚  â””â”€ type_validator.py
â”œâ”€ execution_monitoring/
â”‚  â”œâ”€ real_time_monitor.py
â”‚  â””â”€ anomaly_detector.py
â”œâ”€ output_validation/
â”‚  â”œâ”€ output_checker.py
â”‚  â””â”€ phi_scanner.py
â””â”€ audit/
   â”œâ”€ audit_logger.py
   â””â”€ compliance_reporter.py
```

### Module 9: Evaluation Harness
```
geisinger-evaluation/
â”œâ”€ golden_tests/
â”‚  â”œâ”€ clinical_golden_tests.py
â”‚  â”œâ”€ legal_golden_tests.py
â”‚  â””â”€ product_golden_tests.py
â”œâ”€ safety_tests/
â”‚  â”œâ”€ prompt_injection_tests.py
â”‚  â”œâ”€ data_leakage_tests.py
â”‚  â””â”€ authorization_tests.py
â”œâ”€ regression_tests/
â”‚  â”œâ”€ regression_suite.py
â”‚  â””â”€ bug_to_test_mapper.py
â”œâ”€ performance_tests/
â”‚  â”œâ”€ latency_benchmarks.py
â”‚  â””â”€ resource_benchmarks.py
â”œâ”€ red_team/
â”‚  â”œâ”€ adversarial_prompting.py
â”‚  â””â”€ tool_abuse_tests.py
â””â”€ shadow_testing/
   â”œâ”€ shadow_runner.py
   â””â”€ response_comparator.py
```

### Module 10: Observability & Metrics
```
geisinger-observability/
â”œâ”€ metrics/
â”‚  â”œâ”€ metrics_collector.py
â”‚  â”œâ”€ metrics_aggregator.py
â”‚  â””â”€ metrics_exporter.py
â”œâ”€ tracing/
â”‚  â”œâ”€ execution_tracer.py
â”‚  â””â”€ decision_tracer.py
â”œâ”€ dashboard/
â”‚  â”œâ”€ real_time_dashboard.py
â”‚  â””â”€ trend_analyzer.py
â””â”€ alerting/
   â”œâ”€ alert_manager.py
   â””â”€ escalation_handler.py
```

### Module 11: Cross-Cutting Concerns
```
geisinger-shared/
â”œâ”€ security/
â”‚  â”œâ”€ rbac_manager.py
â”‚  â”œâ”€ consent_manager.py
â”‚  â”œâ”€ encryption_service.py
â”‚  â””â”€ token_manager.py
â”œâ”€ config/
â”‚  â”œâ”€ environment_config.py
â”‚  â””â”€ feature_flags.py
â””â”€ utils/
   â”œâ”€ token_counter.py
   â”œâ”€ error_handler.py
   â””â”€ retry_logic.py
```

---

## Deployment Considerations

### Infrastructure Requirements

#### Compute
```
Production Agent Runtime:
â”œâ”€ CPU: 4-8 cores
â”œâ”€ Memory: 16-32 GB
â”œâ”€ GPU: Optional (for model inference)
â””â”€ Instances: 3+ (HA)

MCP Servers (per server):
â”œâ”€ CPU: 2-4 cores
â”œâ”€ Memory: 4-8 GB
â””â”€ Instances: 2+ (HA)

Evaluation Harness:
â”œâ”€ CPU: 8-16 cores
â”œâ”€ Memory: 32-64 GB
â””â”€ Instances: Dedicated test cluster
```

#### Storage
```
Short-Term (Redis/Memory):
â”œâ”€ Working Memory: 10 GB per agent
â”œâ”€ Tool Results Cache: 5 GB per agent
â””â”€ Session State: 2 GB per agent

Long-Term (Database):
â”œâ”€ Persistent Memory: PostgreSQL
â”œâ”€ Audit Logs: Time-series DB
â”œâ”€ Knowledge Base: Document store
â””â”€ Metrics: Prometheus/InfluxDB
```

#### Network
```
Internal:
â”œâ”€ Agent â†” MCP Servers: Low latency (<10ms)
â”œâ”€ MCP Servers â†” Enterprise Systems: <50ms
â””â”€ Agent â†” Memory Store: <5ms

External:
â””â”€ Anthropic API: Internet access (for model inference)
```

### Deployment Topology

#### Option 1: Kubernetes (Recommended)
```yaml
Namespaces:
â”œâ”€ geisinger-agents-prod
â”œâ”€ geisinger-agents-staging
â”œâ”€ geisinger-mcp-servers
â”œâ”€ geisinger-eval-harness
â””â”€ geisinger-monitoring

Components:
â”œâ”€ Agent Pods (StatefulSet, 3 replicas)
â”œâ”€ MCP Server Pods (Deployment, 2+ replicas each)
â”œâ”€ Memory Store (Redis, 3 replicas)
â”œâ”€ Knowledge Base (PostgreSQL, HA)
â””â”€ Metrics (Prometheus + Grafana)
```

#### Option 2: Azure (Native)
```
Resource Groups:
â”œâ”€ RG-Geisinger-Agents-Prod
â”œâ”€ RG-Geisinger-MCP-Servers
â””â”€ RG-Geisinger-Data

Services:
â”œâ”€ Azure Container Instances (Agents)
â”œâ”€ Azure Kubernetes Service (MCP Servers)
â”œâ”€ Azure Cache for Redis (Memory)
â”œâ”€ Azure Database for PostgreSQL (Knowledge)
â””â”€ Azure Monitor (Observability)
```

### Security Zones

```
Zone 1: Public (Internet-facing)
â””â”€ Conversational UI (with auth)

Zone 2: Application (DMZ)
â”œâ”€ Agent Runtime
â””â”€ Interaction Layer

Zone 3: Services (Internal)
â”œâ”€ MCP Clients
â”œâ”€ Memory Management
â””â”€ Knowledge Management

Zone 4: Integration (Restricted)
â”œâ”€ MCP Servers
â””â”€ Tool Execution

Zone 5: Data (Highly Restricted)
â”œâ”€ Enterprise Systems (EHR, etc.)
â””â”€ PHI/PII Data Stores

Firewall Rules:
â”œâ”€ Zone 1 â†’ Zone 2: HTTPS only
â”œâ”€ Zone 2 â†’ Zone 3: Internal API
â”œâ”€ Zone 3 â†’ Zone 4: MCP Protocol
â”œâ”€ Zone 4 â†’ Zone 5: Encrypted, audited
â””â”€ No Zone 5 â†’ Any (no outbound)
```

### Environment Strategy

```
Development:
â”œâ”€ Synthetic data only
â”œâ”€ Mock MCP servers
â”œâ”€ Local evaluation harness
â””â”€ No PHI

Staging:
â”œâ”€ De-identified data
â”œâ”€ Partial MCP servers (non-prod endpoints)
â”œâ”€ Full evaluation harness
â””â”€ Shadow testing enabled

Production:
â”œâ”€ Real data (PHI)
â”œâ”€ All MCP servers
â”œâ”€ Continuous evaluation
â”œâ”€ Full monitoring & alerting
â””â”€ HITL enforced
```

---

## Deployment Phases (Recommended)

### Phase 1: Foundation (Months 1-2)
**Goal**: Build testable core components

```
Deliverables:
â”œâ”€ Module 1: Core Agent Runtime
â”œâ”€ Module 2: Tool Framework (basic)
â”œâ”€ Module 7: HITL System (Tier 3/4 only)
â”œâ”€ Module 8: Safety & Guardrails (basic)
â””â”€ Module 9: Evaluation Harness (golden tests)

First Domain: Product Management (non-PHI)
â”œâ”€ JIRA MCP Server
â”œâ”€ GitHub MCP Server
â””â”€ Product Management BluePrints

Validation:
â””â”€ Run 10 golden tests, achieve 90% pass rate
```

### Phase 2: Memory & Context (Month 3)
**Goal**: Implement sophisticated context management

```
Deliverables:
â”œâ”€ Working memory implementation
â”œâ”€ Long-term memory store
â”œâ”€ Context budget management
â””â”€ Auto-compaction service

Validation:
â”œâ”€ Handle 50-turn conversations
â”œâ”€ Stay within 200K token budget
â””â”€ Maintain >90% recall after compaction
```

### Phase 3: Enhanced Tools & MCP (Month 4)
**Goal**: Scale MCP integration

```
Deliverables:
â”œâ”€ 5+ MCP servers operational
â”œâ”€ Tool discovery engine
â”œâ”€ Parallel execution
â””â”€ Advanced tool registry

Validation:
â”œâ”€ Execute 3+ tools in parallel
â”œâ”€ Tool discovery works for new additions
â””â”€ All MCP servers pass safety tests
```

### Phase 4: Clinical Read-Only (Months 5-6)
**Goal**: Deploy first clinical agent (no writes)

```
Deliverables:
â”œâ”€ EHR MCP Server (read-only)
â”œâ”€ Clinical BluePrints
â”œâ”€ Enhanced PHI protection
â””â”€ Clinical golden test suite

Constraints:
â”œâ”€ Zero write operations
â”œâ”€ Strict citations required
â”œâ”€ Tier 2-3 HITL only

Validation:
â””â”€ Zero PHI leakage in 1000 test runs
```

### Phase 5: Multi-Agent & Legal (Month 7)
**Goal**: Orchestration and second domain

```
Deliverables:
â”œâ”€ Multi-agent orchestrator
â”œâ”€ Legal domain MCP servers
â”œâ”€ Legal BluePrints
â””â”€ Parallel task execution

Use Case: Contract analysis
â”œâ”€ Spawn 3 workers (compliance, risk, financial)
â”œâ”€ Integrate results
â””â”€ Tier 3 approval for recommendations

Validation:
â””â”€ 2x faster than single-agent approach
```

### Phase 6: Limited Writes (Month 8-9)
**Goal**: Enable reversible write operations

```
Deliverables:
â”œâ”€ Write-capable MCP servers
â”œâ”€ Enhanced self-verification
â”œâ”€ Rollback mechanisms
â””â”€ Tier 4 HITL enforcement

Allowed Actions:
â”œâ”€ Draft document generation
â”œâ”€ Calendar scheduling
â”œâ”€ Email sending
â””â”€ Ticket creation

Constraints:
â”œâ”€ All writes require approval
â”œâ”€ Rollback tested for each action
â””â”€ Enhanced audit logging

Validation:
â””â”€ Zero unauthorized writes in shadow testing
```

### Phase 7: Production Scale (Month 10-12)
**Goal**: Enterprise-wide deployment

```
Deliverables:
â”œâ”€ HA deployment (Kubernetes)
â”œâ”€ Full observability stack
â”œâ”€ Continuous evaluation pipeline
â”œâ”€ Self-service agent creation
â””â”€ Performance optimization

Scale Targets:
â”œâ”€ Support 100+ concurrent agents
â”œâ”€ Handle 10,000 requests/day
â”œâ”€ Maintain <2s p99 latency
â””â”€ >99.9% availability

Validation:
â””â”€ Pass load testing at 2x expected traffic
```

---

## Success Criteria

### Technical Metrics
```
Performance:
â”œâ”€ Agent Loop Latency: p99 < 15s
â”œâ”€ Tool Execution: p99 < 1s
â”œâ”€ Context Management: Compaction < 500ms
â””â”€ Throughput: >100 QPS

Quality:
â”œâ”€ Golden Test Pass Rate: >90%
â”œâ”€ Self-Verification Accuracy: >85%
â”œâ”€ Regression Rate: <1% per release
â””â”€ User Approval Rate: >80%

Safety:
â”œâ”€ PHI Leakage: 0 incidents
â”œâ”€ Prompt Injection: 100% blocked
â”œâ”€ Authorization Violations: 0 incidents
â””â”€ Audit Trail: 100% coverage
```

### Business Metrics
```
Efficiency:
â”œâ”€ Human Review Reduction: >70%
â”œâ”€ Task Completion Speed: 3x faster
â”œâ”€ Error Rate: <5%
â””â”€ Cost per Task: <$0.50

Adoption:
â”œâ”€ Active Users: 100+ in 6 months
â”œâ”€ Domains Deployed: 3+ in 12 months
â”œâ”€ Satisfaction Score: >4.0/5
â””â”€ Reuse Rate: >70% of components
```

---

## Conclusion

This architecture provides:

âœ… **True Agency** - Agents that dynamically plan and adapt, not workflows
âœ… **Safety by Design** - Multi-layer defense with self-verification
âœ… **Scalable Foundation** - MCP-based integration, reusable components
âœ… **Efficient Oversight** - 70-80% reduction in human review burden
âœ… **Domain Agnostic** - Same core works across clinical, legal, product, etc.
âœ… **Transparent & Auditable** - Complete traceability and explainability
âœ… **Incremental Deployment** - Build, test, learn, scale

**The Geisinger Way**: *Small steps, testable outcomes, trusted agents.*

---

## Next Steps

1. **Review with stakeholders** - AVP of AI, Security, Compliance, Domain leads
2. **Prioritize domains** - Start with non-PHI (Product) then Clinical read-only
3. **Establish tooling** - ADO templates, CI/CD pipelines, monitoring dashboards
4. **Build core team** - Assign owners per module (see Implementation Modules)
5. **Create 30/60/90 roadmap** - Break down phases into concrete epics
6. **Set up governance** - Weekly architecture reviews, monthly red team tests

**Key Principle**: Ship small, testable slices every 2 weeks. Each must pass golden tests, safety tests, and gain human approval before proceeding.

---

**Document Version**: 1.0
**Last Updated**: January 2025
**Owner**: Senior Architect AI
**Review Cycle**: Bi-weekly with AVP of AI
