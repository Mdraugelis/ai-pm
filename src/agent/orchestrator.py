"""
Agent Orchestrator
Geisinger AI Product Manager Agent - Layer 1: Core Agent Runtime

Main agent loop controller implementing the Geisinger Agentic Architecture.

Agent Loop: Gather â†’ Plan â†’ Act â†’ Verify â†’ Iterate

IMPORTANT ARCHITECTURE:
- SDK Integration: Used ONLY for LLM interface (planning, responses)
- Geisinger Framework: Handles orchestration, verification, HITL, memory
- This orchestrator coordinates ALL layers, SDK is just one component

Following patterns from geisinger-sdk-integrator agent guidance.
"""

import asyncio
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

import structlog

from src.agent.llm_interface import LLMInterface
from src.tools.sdk_tool_adapter import SDKToolAdapter

logger = structlog.get_logger(__name__)


# ============================================================================
# Data Structures
# ============================================================================


@dataclass
class Task:
    """Task for agent to execute"""

    description: str
    initiative_id: Optional[str] = None
    domain: str = "product_management"
    metadata: Dict[str, Any] = None


@dataclass
class Plan:
    """Execution plan generated by agent"""

    steps: List[Dict[str, Any]]
    confidence: float
    reasoning: str
    requires_approval: bool = False


@dataclass
class AgentResponse:
    """Final agent response"""

    status: str  # SUCCESS, FAILED, ESCALATED, MAX_ITERATIONS
    result: Optional[Dict[str, Any]] = None
    verification: Optional[Dict[str, Any]] = None
    trace: List[Dict[str, Any]] = None
    reasoning: List[str] = None
    requires_approval: bool = False
    hitl_tier: Optional[str] = None


# ============================================================================
# Agent Orchestrator
# ============================================================================


class AgentOrchestrator:
    """
    Main agent loop controller

    Implements the Geisinger agent pattern:
    1. GATHER context (from memory, databases, blueprints)
    2. PLAN approach (using LLM via SDK)
    3. ACT (execute tools)
    4. VERIFY (self-check against policies)
    5. ITERATE or ESCALATE

    SDK is used ONLY in step 2 (PLAN) for LLM calls.
    All other logic is pure Geisinger framework.

    Example:
        >>> orchestrator = AgentOrchestrator(config)
        >>> result = await orchestrator.execute_task(task)
    """

    def __init__(
        self,
        config: Dict[str, Any],
        blueprint_loader: Optional[Any] = None,
        tool_registry: Optional[Any] = None,
        memory_manager: Optional[Any] = None,
    ):
        """
        Initialize orchestrator

        Args:
            config: Agent configuration
            blueprint_loader: Blueprint loader (Layer 7: Knowledge)
            tool_registry: Tool registry (Layer 3: Tools)
            memory_manager: Memory manager (Layer 2: Memory)
        """
        self.config = config
        self.blueprint_loader = blueprint_loader
        self.tool_registry = tool_registry
        self.memory_manager = memory_manager

        # Initialize LLM interface (uses SDK internally)
        self.llm = LLMInterface(config, blueprint_loader, tool_registry)

        # Initialize tool adapter for SDK bridge
        self.tool_adapter = SDKToolAdapter()

        # Configuration
        self.max_iterations = config.get("max_iterations", 10)
        self.confidence_threshold = config.get("confidence_threshold", 0.7)

        logger.info(
            "Agent orchestrator initialized",
            max_iterations=self.max_iterations,
            confidence_threshold=self.confidence_threshold,
        )

    async def execute_task(self, task: Task) -> AgentResponse:
        """
        Main agent loop

        Executes: Gather â†’ Plan â†’ Act â†’ Verify â†’ Iterate

        Args:
            task: Task to execute

        Returns:
            AgentResponse with result and trace
        """
        logger.info("Starting task execution", task_description=task.description)

        # Initialize execution trace
        trace = []
        reasoning = []

        # Load blueprints (GATHER step - Geisinger layer)
        meta_blueprint = await self._load_meta_blueprint()
        domain_blueprint = await self._load_domain_blueprint(task.domain)

        # Initialize context (GATHER step - Geisinger layer)
        context = {
            "task": task,
            "meta_blueprint": meta_blueprint,
            "domain_blueprint": domain_blueprint,
            "iteration": 0,
            "conversation": [],
            "tool_results": [],
            "previous_attempts": [],
        }

        # Agent loop
        for iteration in range(self.max_iterations):
            context["iteration"] = iteration

            logger.info("Agent iteration", iteration=iteration)

            # 1. GATHER context (Geisinger layer)
            gathered_context = await self._gather_context(context)
            trace.append(
                {
                    "step": "gather",
                    "iteration": iteration,
                    "context_summary": self._summarize_context(gathered_context),
                }
            )

            # 2. PLAN approach (SDK layer - LLM call)
            plan = await self._create_plan(task, gathered_context)
            trace.append(
                {
                    "step": "plan",
                    "iteration": iteration,
                    "plan": plan,
                    "confidence": plan.confidence,
                }
            )

            reasoning.append(
                f"Iteration {iteration}: {plan.reasoning} (confidence: {plan.confidence:.0%})"
            )

            # 3. ACT - Execute plan (Geisinger layer)
            execution_result = await self._execute_plan(plan, gathered_context)
            trace.append(
                {
                    "step": "execute",
                    "iteration": iteration,
                    "result": execution_result,
                }
            )

            # 4. VERIFY - Self-check (Geisinger layer)
            verification = await self._verify_result(
                execution_result, plan, gathered_context
            )
            trace.append(
                {
                    "step": "verify",
                    "iteration": iteration,
                    "verification": verification,
                }
            )

            # 5. ITERATE/ADAPT or COMPLETE
            if verification.get("complete", False):
                # Task complete!
                logger.info(
                    "Task completed successfully",
                    iterations=iteration + 1,
                    confidence=verification.get("confidence"),
                )

                return AgentResponse(
                    status="SUCCESS",
                    result=execution_result,
                    verification=verification,
                    trace=trace,
                    reasoning=reasoning,
                    requires_approval=verification.get("requires_approval", False),
                    hitl_tier=verification.get("hitl_tier"),
                )

            elif verification.get("should_escalate", False):
                # Escalate to human
                logger.warning(
                    "Task escalated",
                    reason=verification.get("escalation_reason"),
                    iterations=iteration + 1,
                )

                return AgentResponse(
                    status="ESCALATED",
                    result=execution_result,
                    verification=verification,
                    trace=trace,
                    reasoning=reasoning,
                )

            else:
                # Adapt and continue
                context = self._adapt_context(context, verification)
                logger.info("Adapting plan", issues=verification.get("issues"))

        # Max iterations reached
        logger.error("Max iterations reached", max_iterations=self.max_iterations)

        return AgentResponse(
            status="MAX_ITERATIONS",
            trace=trace,
            reasoning=reasoning,
        )

    async def _gather_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        GATHER step: Collect all relevant context

        This is PURE Geisinger - no SDK involved.
        Gathers from memory, databases, blueprints, etc.

        Enhanced to extract and prioritize user-uploaded blueprints.

        Args:
            context: Current context

        Returns:
            Enriched context with organized blueprint knowledge
        """
        # Placeholder - will connect to memory manager, database, etc.
        gathered = context.copy()

        # Add timestamp
        gathered["timestamp"] = datetime.now().isoformat()

        # Add available tools (from tool registry)
        if self.tool_registry:
            gathered["available_tools"] = self.tool_registry.get_tool_ids()

        # Organize blueprint knowledge with user blueprints prioritized
        blueprints = context.get("blueprints", {})
        if blueprints:
            # Extract user blueprints if available
            user_blueprints = blueprints.get("user_blueprints", {})

            # Create organized blueprint context
            gathered["blueprint_context"] = {
                "has_user_blueprints": bool(user_blueprints.get("count", 0)),
                "user_blueprint_count": user_blueprints.get("count", 0),
                "user_blueprint_subtypes": list(user_blueprints.get("by_subtype", {}).keys()),
                "all_blueprints": blueprints
            }

            # Log blueprint availability
            logger.info(
                "Blueprint context gathered",
                user_blueprints=user_blueprints.get("count", 0),
                subtypes=list(user_blueprints.get("by_subtype", {}).keys()) if user_blueprints else []
            )

        return gathered

    async def _create_plan(self, task: Task, context: Dict[str, Any]) -> Plan:
        """
        PLAN step: Generate execution plan

        This USES SDK via LLMInterface for the LLM call,
        but planning logic is Geisinger's.

        Args:
            task: Task to plan for
            context: Execution context

        Returns:
            Plan object
        """
        # Call LLM via SDK integration (SDK's job)
        plan_dict = await self.llm.generate_plan(
            {"description": task.description, "initiative_id": task.initiative_id},
            context,
        )

        # Convert to Geisinger Plan (Geisinger's job)
        plan = Plan(
            steps=plan_dict.get("steps", []),
            confidence=plan_dict.get("confidence", 0.5),
            reasoning=plan_dict.get("reasoning", "LLM generated plan"),
            requires_approval=plan_dict.get("confidence", 0.5)
            < self.confidence_threshold,
        )

        return plan

    async def _execute_plan(
        self, plan: Plan, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ACT step: Execute the plan

        For specialized modes, uses LLM to generate content based on plan.
        For tool execution, will use tool registry (future implementation).

        Args:
            plan: Plan to execute
            context: Execution context

        Returns:
            Execution result with generated content
        """
        logger.info("Executing plan", steps=len(plan.steps))

        # Build execution prompt from plan
        execution_prompt = self._build_execution_prompt(plan, context)

        # Use LLM to execute the plan
        try:
            response = await self.llm.ask_question(execution_prompt, context)

            result = {
                "status": "success",
                "response": response,
                "plan_steps_executed": len(plan.steps),
            }

            logger.info(
                "Plan executed successfully",
                steps=len(plan.steps),
                response_length=len(response),
            )

            return result

        except Exception as e:
            logger.error("Plan execution failed", error=str(e))
            return {
                "status": "failed",
                "error": str(e),
                "plan_steps_executed": 0,
            }

    def _build_execution_prompt(
        self, plan: Plan, context: Dict[str, Any]
    ) -> str:
        """
        Build execution prompt from plan and context

        Args:
            plan: Plan to execute
            context: Execution context

        Returns:
            Prompt for LLM to execute the plan
        """
        # Extract task description
        task = context.get("task")
        task_description = task.description if task else "Complete the task"

        # Extract mode and documents
        mode = context.get("mode", "general")
        documents = context.get("documents", [])
        blueprints = context.get("blueprints", {})

        # Build prompt
        prompt = f"""Task: {task_description}

Mode: {mode}

Plan to execute:
{plan.reasoning}

Steps:
"""

        # Add plan steps
        for i, step in enumerate(plan.steps, 1):
            prompt += f"{i}. {step.get('action', 'Execute step')}\n"

        # Add context information
        if documents:
            prompt += f"\n\nContext documents available: {len(documents)}\n"
            for doc in documents:
                prompt += f"- {doc.get('type')}: {len(doc.get('content', ''))} characters\n"

        # Add blueprints as guidance with user blueprints prioritized
        if blueprints:
            prompt += "\n\n=== STRATEGIC GUIDANCE FROM BLUEPRINTS ===\n"

            # Check for user-uploaded blueprints
            user_blueprints = blueprints.get("user_blueprints", {})
            has_user_blueprints = user_blueprints.get("count", 0) > 0

            if has_user_blueprints:
                prompt += "\nðŸŽ¯ **USER-UPLOADED KNOWLEDGE** (PRIORITY - Use these first!):\n"
                by_subtype = user_blueprints.get("by_subtype", {})

                # Organize by subtype
                for subtype, docs in by_subtype.items():
                    prompt += f"\n  {subtype.upper()}S ({len(docs)} documents):\n"
                    for doc in docs[:3]:  # Show first 3 per type
                        filename = doc.get("filename", "Unknown")
                        metadata = doc.get("metadata", {})

                        # Parse metadata if string
                        if isinstance(metadata, str):
                            try:
                                import json
                                metadata = json.loads(metadata)
                            except:
                                metadata = {}

                        summary = metadata.get("summary", "")
                        key_concepts = metadata.get("key_concepts", [])

                        prompt += f"    - {filename}\n"
                        if summary:
                            prompt += f"      Summary: {summary[:150]}...\n"
                        if key_concepts:
                            prompt += f"      Key concepts: {', '.join(key_concepts[:5])}\n"

                    if len(docs) > 3:
                        prompt += f"    ... and {len(docs) - 3} more {subtype}s\n"

                prompt += "\n  âš ï¸ IMPORTANT: User-uploaded blueprints take precedence over default guidance.\n"

            # Add YAML/default blueprints
            prompt += "\nðŸ“š **DEFAULT KNOWLEDGE BASE**:\n"

            # Domain knowledge
            if "domain_knowledge" in blueprints:
                domain = blueprints["domain_knowledge"]
                policies = domain.get("policies", [])
                guidelines = domain.get("guidelines", [])

                if policies:
                    # Show first few user policies if any
                    user_policies = [p for p in policies if p.get("source") == "user_upload"]
                    yaml_policies = [p for p in policies if p.get("source") != "user_upload"]

                    if yaml_policies:
                        prompt += f"  - {len(yaml_policies)} standard policies\n"

                if guidelines:
                    user_guidelines = [g for g in guidelines if g.get("source") == "user_upload"]
                    yaml_guidelines = [g for g in guidelines if g.get("source") != "user_upload"]

                    if yaml_guidelines:
                        prompt += f"  - {len(yaml_guidelines)} standard guidelines\n"

            # Other blueprints (workflow, templates)
            other_blueprints = [k for k in blueprints.keys()
                              if k not in ["user_blueprints", "domain_knowledge", "meta"]]
            if other_blueprints:
                for bp_name in other_blueprints:
                    prompt += f"  - {bp_name}\n"

        # Add mode-specific instructions
        if mode == "ai_discovery":
            prompt += """

Please generate a comprehensive AI Discovery Form response following the guidance from the blueprints.
Include all relevant sections and provide detailed, actionable information.
Use markdown formatting for readability.
"""
        elif mode == "risk_assessment":
            prompt += """

Please provide a detailed risk assessment following the governance guidelines.
Include risk identification, impact analysis, and mitigation recommendations.
Use markdown formatting for clarity.
"""
        elif mode == "poc_planning":
            prompt += """

Please create a detailed proof-of-concept plan.
Include timeline, resources, success criteria, and milestones.
Use markdown formatting for readability.
"""

        prompt += "\n\nPlease execute this plan now and provide the complete output."

        return prompt

    async def _verify_result(
        self, result: Dict[str, Any], plan: Plan, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        VERIFY step: Self-check result

        This is PURE Geisinger - verification against policies.
        No SDK involved.

        Args:
            result: Execution result
            plan: Original plan
            context: Execution context

        Returns:
            Verification result with complete/escalate decision
        """
        # Placeholder - will use SelfVerifier from governance layer
        verification = {
            "passed": True,
            "complete": True,  # For now, complete immediately
            "confidence": plan.confidence,
            "checks": [],
            "requires_approval": plan.confidence < self.confidence_threshold,
            "hitl_tier": "TIER_3" if plan.requires_approval else "TIER_1",
        }

        return verification

    def _adapt_context(
        self, context: Dict[str, Any], verification: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Adapt context based on verification feedback

        Args:
            context: Current context
            verification: Verification result

        Returns:
            Adapted context
        """
        adapted = context.copy()

        # Add verification issues to previous attempts
        if "issues" in verification:
            adapted.setdefault("previous_attempts", []).append(verification["issues"])

        return adapted

    async def _load_meta_blueprint(self) -> Dict[str, Any]:
        """Load meta-blueprint (universal policies)"""
        if self.blueprint_loader:
            return self.blueprint_loader.load_meta_blueprint()
        return {}

    async def _load_domain_blueprint(self, domain: str) -> Dict[str, Any]:
        """Load domain-specific blueprint"""
        if self.blueprint_loader:
            return self.blueprint_loader.load_domain_blueprint(domain)
        return {}

    async def execute_task_with_context(
        self,
        task: Task,
        external_context: Dict[str, Any],
    ) -> AgentResponse:
        """
        Execute task with external context from ConversationalAgent

        Same agent loop as execute_task(), but accepts additional context:
        - Conversation history
        - User-uploaded documents
        - Mode-specific blueprints

        Args:
            task: Task to execute
            external_context: Context from ConversationalAgent containing:
                - conversation: List of conversation turns
                - mode: Operating mode
                - documents: User-uploaded documents
                - blueprints: Mode-specific blueprints as knowledge

        Returns:
            AgentResponse with result and trace
        """
        logger.info(
            "Starting task with external context",
            task_description=task.description,
            mode=external_context.get("mode"),
            conversation_turns=len(external_context.get("conversation", [])),
            documents_count=len(external_context.get("documents", [])),
        )

        # Merge external context with standard context
        context = {
            **external_context,
            "task": task,
            "iteration": 0,
            "tool_results": [],
            "previous_attempts": [],
        }

        # Load blueprints if not provided
        if "meta_blueprint" not in context:
            context["meta_blueprint"] = await self._load_meta_blueprint()
        if "domain_blueprint" not in context:
            context["domain_blueprint"] = await self._load_domain_blueprint(
                task.domain
            )

        # Initialize execution trace
        trace = []
        reasoning = []

        # Agent loop (same as execute_task)
        for iteration in range(self.max_iterations):
            context["iteration"] = iteration

            logger.info("Agent iteration", iteration=iteration)

            # 1. GATHER context
            gathered_context = await self._gather_context(context)
            trace.append({
                "step": "gather",
                "iteration": iteration,
                "context_summary": self._summarize_context(gathered_context),
            })

            # 2. PLAN approach (LLM decides what to do)
            plan = await self._create_plan(task, gathered_context)
            trace.append({
                "step": "plan",
                "iteration": iteration,
                "plan": plan,
                "confidence": plan.confidence,
            })

            reasoning.append(
                f"Iteration {iteration}: {plan.reasoning} (confidence: {plan.confidence:.0%})"
            )

            # 3. ACT - Execute plan
            execution_result = await self._execute_plan(plan, gathered_context)
            trace.append({
                "step": "execute",
                "iteration": iteration,
                "result": execution_result,
            })

            # 4. VERIFY - Self-check
            verification = await self._verify_result(
                execution_result, plan, gathered_context
            )
            trace.append({
                "step": "verify",
                "iteration": iteration,
                "verification": verification,
            })

            # 5. ITERATE/ADAPT or COMPLETE
            if verification.get("complete", False):
                # Task complete!
                logger.info(
                    "Task completed successfully",
                    iterations=iteration + 1,
                    confidence=verification.get("confidence"),
                )

                return AgentResponse(
                    status="SUCCESS",
                    result=execution_result,
                    verification=verification,
                    trace=trace,
                    reasoning=reasoning,
                    requires_approval=verification.get("requires_approval", False),
                    hitl_tier=verification.get("hitl_tier"),
                )

            elif verification.get("should_escalate", False):
                # Escalate to human
                logger.warning(
                    "Task escalated",
                    reason=verification.get("escalation_reason"),
                    iterations=iteration + 1,
                )

                return AgentResponse(
                    status="ESCALATED",
                    result=execution_result,
                    verification=verification,
                    trace=trace,
                    reasoning=reasoning,
                )

            else:
                # Adapt and continue
                context = self._adapt_context(context, verification)
                logger.info("Adapting plan", issues=verification.get("issues"))

        # Max iterations reached
        logger.error("Max iterations reached", max_iterations=self.max_iterations)

        return AgentResponse(
            status="MAX_ITERATIONS",
            trace=trace,
            reasoning=reasoning,
        )

    def _summarize_context(self, context: Dict[str, Any]) -> str:
        """Create context summary for trace"""
        return f"Iteration {context.get('iteration', 0)}, {len(context.get('tool_results', []))} tool results"


# ============================================================================
# Module Exports
# ============================================================================

__all__ = [
    "AgentOrchestrator",
    "Task",
    "Plan",
    "AgentResponse",
]
